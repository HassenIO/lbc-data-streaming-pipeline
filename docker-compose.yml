version: '3'

services:
  zookeeper:
    image: quay.io/debezium/zookeeper:2.1
    ports:
      - 2181:2181
      - 2888:2888
      - 3888:3888
    # networks:
    #   - lbc-network

  kafka:
    image: quay.io/debezium/kafka:2.1
    ports:
      - 9092:9092
    links:
      - zookeeper
    environment:
      ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_CREATE_TOPICS: dbserver1.inventory.customers:1:1
    # networks:
    #   - lbc-network
  
  kafdrop: # UI to monitor kafka (optional)
    image: obsidiandynamics/kafdrop:latest
    ports:
      - 9000:9000
    environment:
      KAFKA_BROKERCONNECT: kafka:9092
    depends_on:
      - kafka

  postgres:
    image: quay.io/debezium/example-postgres:2.1
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    # networks:
    #   - lbc-network

  connect: # Used by Debezium to connect postgres to kafka
    image: quay.io/debezium/connect:2.1
    ports:
      - 8083:8083
    links:
      - kafka
      - postgres
    environment:
      GROUP_ID: 1
      BOOTSTRAP_SERVERS: kafka:9092
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
    # networks:
    #   - lbc-network

  mongodb:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - ./data/mongo:/data/db

  app: # A jupyter Notebook to run the PySpark code
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./notebooks:/home/jovyan/notebooks
    ports:
      - "8888:8888" # Jupyter lab UI
      - "4040:4040" # Spark UI
    depends_on:
      - postgres
      - kafka
      - mongodb
    environment:
      - KAFKA_BROKER=kafka:9092

# networks:
#   lbc-network:
